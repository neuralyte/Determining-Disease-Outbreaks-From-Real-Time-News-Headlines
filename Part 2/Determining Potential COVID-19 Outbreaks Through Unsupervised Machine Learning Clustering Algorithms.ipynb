{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering and Plotting Headlines</h1>\n",
    "\n",
    "<h2>Step 1: Reading in Headlines</h2>\n",
    "\n",
    "The file, \"headline.txt\" is read in and appended to a headlines array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8555\n"
     ]
    }
   ],
   "source": [
    "file1 = open('headline.txt', 'r') \n",
    "file2 = open('headline_world.txt', 'r')\n",
    "headline_lines = file1.readlines() \n",
    "headline_world_lines = file2.readlines()\n",
    "  \n",
    "headlines = []\n",
    "for line in headline_lines: \n",
    "    headlines.append(line.strip())\n",
    "for line in headline_world_lines:\n",
    "    headlines.append(line.strip())\n",
    "\n",
    "headlines = list(set(headlines))\n",
    "print(len(headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 2: Refining the Headlines</h2>\n",
    "\n",
    "The news channel name is removed from the end of the title so the data is not skewed (e.g. \"Los Angeles Times\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To combat coronavirus, Sea-Tac Airport is stretching lines, blocking seats — but some ask what took so long', 'US Coronavirus Updates: Conspiracy-Theory Video Shows Challenges for Tech; Fauci Says Outbreak ‘Not Under Control’', \"'It was only a couple of hours': 18 members of Texas family infected with COVID-19 after surprise party\", 'Coronavirus', 'White House: Trump’s ‘Coronavirus Test Slowdown’ Comments Were Tongue-in-Cheek']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(headlines)):\n",
    "    headline = str(headlines[i])\n",
    "    size = len(headline)-2\n",
    "    if (size <= 0):\n",
    "        continue\n",
    "    while (size != 0):\n",
    "        if (headline[size-1:size+2] == \" | \" or headline[size-1:size+2] == ' - ' or headline[size-1:size+2] == ' – '):\n",
    "            break\n",
    "        size-=1\n",
    "    if (size != 0):\n",
    "        headlines[i] = headline[:size-1]\n",
    "\n",
    "print(headlines[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 3: Importing Libraries</h2>\n",
    "\n",
    "Each of the libraries imported provides a sizable contribution to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geonamescache\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from math import radians, cos, sin, sqrt, asin\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 4: Extracting Country, County, and State Names</h2>\n",
    "\n",
    "First, using the geonamescache library, the country names are extracted. A dictionary is created that stores the country name and its respective count; in the beginning, it is initialized to zero as there are no headlines parsed. A subsequent process was performed with the county names and the state names; for the state names, the state with its respective code was stored as well (e.g. California and CA were both stored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnc = geonamescache.GeonamesCache()\n",
    "country_names = [k for k in gnc.get_countries_by_names()]\n",
    "\n",
    "# Country\n",
    "country_counter = {}\n",
    "for index in country_names:\n",
    "    country_counter[index] = 0\n",
    "    \n",
    "# County\n",
    "county_names = {}\n",
    "county_counter = {}\n",
    "for index in gnc.get_us_counties():\n",
    "    if (not index['name'] in county_names):\n",
    "        county_names[str(index['name'])] = str(index['state'])\n",
    "        county_counter[str(index['name'])] = 0\n",
    "county_names_sorted = list(county_names.keys())\n",
    "county_names_sorted.sort()\n",
    "\n",
    "# State\n",
    "state_names = []\n",
    "state_keys = {}\n",
    "state_counter = {}\n",
    "state_repository = gnc.get_us_states()\n",
    "for index in list(state_repository.keys()):\n",
    "    state_names.append(state_repository[index]['name'])\n",
    "    state_keys[state_repository[index]['code']] = state_repository[index]['name']\n",
    "    state_counter[state_repository[index]['name']] = 0\n",
    "    \n",
    "print(\"Here are the first five countries extracted: \")\n",
    "print(country_names[:5])\n",
    "print(\"Here are the first five counties extracted: \")\n",
    "print(county_names_sorted[:5])\n",
    "print(\"Here are the first five states extracted: \")\n",
    "print(state_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 5: Checking the Headlines for Cities, States, Counties, and Countries</h2>\n",
    "\n",
    "First, a dictionary is initialized to contain the respective country, city, state, and county for each headline if there is one. The headlines are normalized (accents and special characters removed) to remove any inconsistencies from different headlines. First, possible cities are determined. The headline is split into an array of words. Substrings from the headline that are determined to be a city using the geonamescache library are added to the poss_cities array. The largest city from poss_cities is chosen as the city that will be added to the dictionary. After much research, it was determined that this would provide the most accurate city for each headline; for example, in the headline \"Coronavirus in New York City kills hundreds\", New York City and York are both plausible cities, but New York City would be chosen as the better city of the two.\n",
    "\n",
    "Next, from the already compiled country names, possible countries are determined from the headlines. This follows a very similar process to that of the cities. However, one key difference is that Georgia and Jersey are purposely excluded as countries; after much research, this was again chosen as the most optimal option, as chosing to keep these countries would cause conflict with the states' results. A similar process is once again followed for counties and states.\n",
    "\n",
    "Lastly, if countries, cities, states, or countries aren't found using the steps shown above, a NaN value is placed in the dataset to signify there is no corresponding value for that. A dataframe with columns 'headline', 'countries', 'cities', 'states', and 'countries' is made from the dictionary for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'headline':[], 'countries':[], 'cities':[], 'states':[], 'counties':[]}\n",
    "\n",
    "for i in headlines:\n",
    "    dictionary['headline'].append(i)\n",
    "    \n",
    "    # Removing Special Characters\n",
    "    i = unidecode.unidecode(i)\n",
    "    words = i.split()\n",
    "    for x in range(len(words)):\n",
    "        if (not words[x][-1].isalnum()):\n",
    "            words[x] = words[x][:-1]\n",
    "    \n",
    "    # Checking whether the phrase extracted is the name of a city\n",
    "    for j in range(len(words)):\n",
    "        poss_cities = []\n",
    "        string = words[j]\n",
    "        if (gnc.get_cities_by_name(words[j]) != []):\n",
    "            poss_cities.append(string)\n",
    "        \n",
    "        for k in range(j+1, len(words)):\n",
    "            string += \" \" + words[k]\n",
    "            if (gnc.get_cities_by_name(string) != []):\n",
    "                poss_cities.append(string)\n",
    "        \n",
    "        if (len(poss_cities) != 0):\n",
    "            dictionary['cities'].append(poss_cities[-1])\n",
    "            break\n",
    "   \n",
    "    # Checking whether the phrase extracted is the name of a country (excluding Georgia and Jersey)\n",
    "    for j in range(len(words)):\n",
    "        poss_countries = []\n",
    "        string = words[j]\n",
    "        if (string in country_names):\n",
    "            poss_countries.append(string)\n",
    "            if (string in list(country_counter.keys())) and (string != \"Georgia\") and (string != \"Jersey\"):\n",
    "                country_counter[string]+=1\n",
    "        \n",
    "        for k in range(j+1, len(words)):\n",
    "            string += \" \" + words[k]\n",
    "            if (string in country_names):\n",
    "                poss_countries.append(string)\n",
    "                if (string in list(country_counter.keys())) and (string != \"Georgia\") and (string != \"Jersey\"):\n",
    "                    country_counter[string]+=1\n",
    "        \n",
    "        if (len(poss_countries) != 0):\n",
    "            dictionary['countries'].append(poss_countries[-1])\n",
    "            break\n",
    "            \n",
    "    # Checking whether the phrase extracted is the name of a county\n",
    "    for j in range(len(words)):\n",
    "        list_of_counties = list(county_names.keys())\n",
    "        poss_counties = []\n",
    "        string = words[j]\n",
    "        if (string in list_of_counties):\n",
    "            poss_counties.append(string)\n",
    "            if string in list(county_counter.keys()):\n",
    "                county_counter[string]+=1\n",
    "        \n",
    "        for k in range(j+1, len(words)):\n",
    "            string += \" \" + words[k]\n",
    "            if (string in list_of_counties):\n",
    "                poss_counties.append(string)\n",
    "                if string in list(county_counter.keys()):\n",
    "                    county_counter[string]+=1\n",
    "        \n",
    "        if (len(poss_counties) != 0):\n",
    "            dictionary['counties'].append(poss_counties[-1])\n",
    "            dictionary['states'].append(state_keys[county_names[poss_counties[-1]]])\n",
    "            break\n",
    "    \n",
    "    # Checking whether the phrase extracted is the name of a state\n",
    "    for j in range(len(words)):\n",
    "        poss_states = []\n",
    "        string = words[j]\n",
    "        if (string in state_names):\n",
    "            poss_states.append(string)\n",
    "            if string in list(state_counter.keys()):\n",
    "                state_counter[string]+=1\n",
    "        \n",
    "        for k in range(j+1, len(words)):\n",
    "            string += \" \" + words[k]\n",
    "            if (string in state_names):\n",
    "                poss_states.append(string)\n",
    "                if string in list(state_counter.keys()):\n",
    "                    state_counter[string]+=1\n",
    "        if (len(poss_states) != 0 and len(dictionary['headline']) != len(dictionary['states'])):\n",
    "            dictionary['states'].append(poss_states[-1])\n",
    "            break\n",
    "    \n",
    "    # Making the country United States if there is a corresponding state or county\n",
    "    if (len(dictionary['headline']) != len(dictionary['countries'])):\n",
    "        if (len(dictionary['headline']) == len(dictionary['states'])) or (len(dictionary['headline']) == len(dictionary['counties'])):\n",
    "            dictionary['countries'].append('United States')\n",
    "            country_counter['United States']+=1\n",
    "        else:\n",
    "            dictionary['countries'].append(np.nan)\n",
    "            \n",
    "    # Appending NaN values if the headline doesn't have a corresponding city, country, state, or county\n",
    "    if (len(dictionary['headline']) != len(dictionary['cities'])):\n",
    "        dictionary['cities'].append(np.nan)\n",
    "    if (len(dictionary['headline']) != len(dictionary['counties'])):\n",
    "        dictionary['counties'].append(np.nan)\n",
    "    if (len(dictionary['headline']) != len(dictionary['states'])):\n",
    "        dictionary['states'].append(np.nan)\n",
    "    while (len(dictionary['states']) > len(dictionary['headline'])):\n",
    "        dictionary['states'].pop()\n",
    "\n",
    "df = pd.DataFrame(data = dictionary)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 6: Determining the Most-Affected Countries, States, and Counties</h2>\n",
    "\n",
    "The dictionaries state_counter, county_counter, and country_counter are converted into dataframes and sorted based on count, to find the most-affected and least-affected countries, states, and counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_df(dict1, value):\n",
    "    dictionary_counter = {value: [], 'Count':[]}\n",
    "    for index in list(dict1.keys()):\n",
    "        dictionary_counter[value].append(index)\n",
    "        dictionary_counter['Count'].append(dict1[index])\n",
    "    df = pd.DataFrame(data = dictionary_counter, columns = [value, 'Count'])\n",
    "    df = df.sort_values(by = ['Count'], ascending = False)\n",
    "    df = df.reset_index()\n",
    "    df.index += 1\n",
    "    del df['index']\n",
    "    return df\n",
    "    \n",
    "df_state = convert_dict_to_df(state_counter, 'State')\n",
    "df_county = convert_dict_to_df(county_counter, 'County')\n",
    "df_country = convert_dict_to_df(country_counter, 'Country')\n",
    "\n",
    "print(df_country.head(10))\n",
    "print()\n",
    "print(df_state.head(10))\n",
    "print()\n",
    "print(df_county.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 7: Identifying Latitude and Longitude for Each Headline</h2>\n",
    "\n",
    "First, the latitudes and longitudes were manually entered for each state. For countries and counties, the latitudes and longitudes were taken from existing datasets and were stored in a dictionary. Next, the dataframe is iterated through to find the latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnc = geonamescache.GeonamesCache()\n",
    "\n",
    "states_lat_long = {\n",
    "    'Alabama': [32.806671, -86.791130],\n",
    "    'Alaska': [61.370716, -152.404419],\n",
    "    'Arizona': [33.729759, -111.431221],\n",
    "    'Arkansas': [34.969704, -92.373123],\n",
    "    'California': [36.116203, -119.618564],\n",
    "    'Colorado': [39.059811, -105.311104], \n",
    "    'Connecticut': [41.597782, -72.755371],\n",
    "    'Delaware': [39.318523, -75.507141],\n",
    "    'District of Columbia': [38.897438, -77.026817],\n",
    "    'Florida': [27.766279, -81.686783],\n",
    "    'Georgia': [33.040619, -83.643074],\n",
    "    'Hawaii': [21.094318, -157.498337],\n",
    "    'Idaho': [44.240459, -114.478828],\n",
    "    'Illinois': [40.349457, -88.986137],\n",
    "    'Indiana': [39.849426, -86.258278],\n",
    "    'Iowa': [42.011539, -93.210526],\n",
    "    'Kansas': [38.526600, -96.726486],\n",
    "    'Kentucky': [37.668140, -84.670067],\n",
    "    'Louisiana': [31.169546, -91.867805],\n",
    "    'Maine': [44.693947, -69.381927],\n",
    "    'Maryland': [39.063946, -76.802101],\n",
    "    'Massachusetts': [42.230171, -71.530106],\n",
    "    'Michigan': [43.326618, -84.536095],\n",
    "    'Minnesota': [45.694454, -93.900192],\n",
    "    'Mississippi': [32.741646, -89.678696],\n",
    "    'Missouri': [38.456085, -92.288368],\n",
    "    'Montana': [46.921925, -110.454353],\n",
    "    'Nebraska': [41.125370, -98.268082],\n",
    "    'Nevada': [38.313515, -117.055374],\n",
    "    'New Hampshire': [43.452492, -71.563896],\n",
    "    'New Jersey': [40.298904, -74.521011],\n",
    "    'New Mexico': [34.840515, -106.248482],\n",
    "    'New York': [42.165726, -74.948051],\n",
    "    'North Carolina': [35.630066, -79.806419],\n",
    "    'North Dakota': [47.528912, -99.784012],\n",
    "    'Ohio': [40.388783, -82.764915],\n",
    "    'Oklahoma': [35.565342, -96.928917],\n",
    "    'Oregon': [44.572021, -122.070938],\n",
    "    'Pennsylvania': [40.590752, -77.209755],\n",
    "    'Rhode Island': [41.680893, -71.511780],\n",
    "    'South Carolina': [33.856892, -80.945007],\n",
    "    'South Dakota': [44.299782, -99.438828],\n",
    "    'Tennessee': [35.747845, -86.692345],\n",
    "    'Texas': [31.054487, -97.563461],\n",
    "    'Utah': [40.150032, -111.862434],\n",
    "    'Vermont': [44.045876, -72.710686],\n",
    "    'Virginia': [37.769337, -78.169968],\n",
    "    'Washington': [47.400902, -121.490494],\n",
    "    'West Virginia': [38.491226, -80.954453],\n",
    "    'Wisconsin': [44.268543, -89.616508],\n",
    "    'Wyoming': [42.755966, -107.302490]\n",
    "}\n",
    "\n",
    "file2 = open('county_lat_long.txt', 'r') \n",
    "counties = file2.readlines()\n",
    "#print(counties)\n",
    "\n",
    "county_data = {}\n",
    "#Strips the newline character \n",
    "for i in range(50, len(counties)):\n",
    "    county_values = counties[i].strip().split('\\t')\n",
    "    county_data[county_values[3]] = [county_values[-2], county_values[-1]]\n",
    "    \n",
    "file2 = open('country_lat_long.txt', 'r') \n",
    "countries = file2.readlines()\n",
    "#print(counties)\n",
    "\n",
    "country_data = {}\n",
    "#Strips the newline character \n",
    "for i in range(len(countries)):\n",
    "    country_values = countries[i].strip().split('\\t')\n",
    "    country_data[country_values[-1]] = [country_values[-3], country_values[-2]]\n",
    "\n",
    "print(\"Here is a subset of the counties:\")\n",
    "print(list(county_data.keys())[:10])\n",
    "print()\n",
    "print(\"Here is a subset of the countries:\")\n",
    "print(list(country_data.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the 'NaN' values are taken care of. The latitude and longitudes were first checked for the city, then county, then state, and then country. Since there are multiple places with the same city name, the city with the highest population was added. Additionally, cities with populations below 50,000 were excluded, because it is improbable that these cities will be mentioned by a reputable news source. The latitude, longitude, and corresponding country code are added to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = []\n",
    "longitude = []\n",
    "country_code = []\n",
    "for index_val in df1.index:\n",
    "    city = df1['cities'][index_val]\n",
    "    state = df1['states'][index_val]\n",
    "    country = df1['countries'][index_val]\n",
    "    county = df1['counties'][index_val]\n",
    "    val = gnc.get_cities_by_name(city)\n",
    "    \n",
    "    # Listing the order of priority: city, county, state, then country\n",
    "    if (city == np.nan or val == []):\n",
    "        if (county in list(county_counter.keys())):\n",
    "            latitude.append(county_data[county][0])\n",
    "            longitude.append(county_data[county][1])\n",
    "        else:\n",
    "            if (state in state_names):\n",
    "                latitude.append(states_lat_long[state][0])\n",
    "                longitude.append(states_lat_long[state][1])\n",
    "            else:\n",
    "                if (country in list(country_data.keys())):\n",
    "                    latitude.append(float(country_data[country][0]))\n",
    "                    longitude.append(float(country_data[country][1]))\n",
    "                else:\n",
    "                    latitude.append(np.nan)\n",
    "                    longitude.append(np.nan)\n",
    "        country_code.append(np.nan)\n",
    "    else:\n",
    "        # Extracting places with a population of more than 50,000 (excluding Latina and York)\n",
    "        maxpop = 0\n",
    "        index = 0\n",
    "        for j in range(len(val)):\n",
    "            keys = [e for e in val[j]]\n",
    "            population = val[j][keys[0]]['population']\n",
    "            if (population > maxpop):\n",
    "                maxpop = population\n",
    "                index = j\n",
    "        keys = [e for e in val[index]]\n",
    "        if (maxpop <= 50000) or (city == \"York\") or (city == \"Latina\" and country != \"Spain\"):\n",
    "            latitude.append(np.nan)\n",
    "            longitude.append(np.nan)\n",
    "            country_code.append(np.nan)\n",
    "            df1.loc[df1.index == index_val, 'cities']=np.nan\n",
    "        else:\n",
    "            latitude.append(val[index][keys[0]]['latitude'])\n",
    "            longitude.append(val[index][keys[0]]['longitude'])\n",
    "            country_code.append(val[index][keys[0]]['countrycode'])\n",
    "        #print(index)\n",
    "        if (str(city) == str(state)):\n",
    "            df1.loc[df1.index == index_val, 'cities']=np.nan\n",
    "        if (str(country) == str(city)):\n",
    "            df1.loc[df1.index == index_val, 'cities']=np.nan\n",
    "            \n",
    "latitude = [float(i) for i in latitude]\n",
    "longitude = [float(i) for i in longitude]\n",
    "\n",
    "df1['latitude'] = latitude\n",
    "df1['longitude'] = longitude\n",
    "df1['countrycode'] = country_code\n",
    "\n",
    "df = df1.dropna(subset = ['latitude', 'longitude'])\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 8: Partitioning the Dataframe</h2> \n",
    "\n",
    "First, the dataframe is divided into four dataframes: df_us (a dataframe containing the headlines pertaining to the United States), df_no_us (a dataframe containing the headlines pertaining to the United States with only 'latitude' and 'longitude' as the columns), df_world (the original dataframe), and df_no_world (the original dataframe with only 'latitude' and 'longitude' as the columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing and Adding to New Dictionaries\n",
    "df_us = {'headline':[], 'cities':[], 'latitude':[], 'counties': [], 'states': [], 'countries': [], 'longitude':[], 'countrycode':[]}\n",
    "df_world = df\n",
    "df_no_us = {'latitude':[], 'longitude':[]}\n",
    "df_no_world = {'latitude':[], 'longitude':[]}\n",
    "\n",
    "for index in df.index:\n",
    "    if (df['countries'][index] == \"United States\"):\n",
    "        for column in list(df.columns):\n",
    "            df_us[column].append(df[column][index])\n",
    "        for column in ['latitude', 'longitude']:\n",
    "            df_no_us[column].append(df[column][index])\n",
    "    for column in ['latitude', 'longitude']:\n",
    "        df_no_world[column].append(df[column][index])\n",
    "            \n",
    "#Converting from Dictionary to Dataframe\n",
    "df_us = pd.DataFrame(data = df_us)\n",
    "df_no_us = pd.DataFrame(data = df_no_us)\n",
    "df_no_world = pd.DataFrame(data = df_no_world)\n",
    "\n",
    "df_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 9: Graphing the Elbow Curve to Determine the Number of Clusters</h2>\n",
    "\n",
    "An elbow curve is graphed to determine the optimal cluster value from a range of one to fifty clusters. After inspection of the elbow curve, thirty clusters was found to be the optimal value for both the US and world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow Curve\n",
    "def elbow_curve(df1):\n",
    "    clusters = range(20, 40)\n",
    "    kmeans_elbow = [KMeans(n_clusters=i) for i in clusters]\n",
    "    score = [kmeans_elbow[i].fit(df1).score(df1) for i in range(len(kmeans_elbow))]\n",
    "    plt.plot(clusters, score)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.savefig('elbowcurve.png')\n",
    "    \n",
    "elbow_curve(df_no_us)\n",
    "elbow_curve(df_no_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step 10: Running the K-Means Clustering Algorithm</h2>\n",
    "\n",
    "Using the sklearn library, a function was created that implements the k-means clustering algorithm on the dataframe's latitude and longitude columns. Additionally, for easier operability, a predefined number of US and world clusters were chosen. The function was run with the US and world dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Algorithm\n",
    "def run_k_means(df1, num_cluster, printGraph):\n",
    "    #Adding to Dataframe\n",
    "    kmeans_elbow = KMeans(n_clusters=num_cluster-1)\n",
    "    df1[\"cluster_label\"] = kmeans_elbow.fit(df1).labels_\n",
    "\n",
    "    if (printGraph):\n",
    "        kmeans = KMeans(n_clusters=num_cluster).fit(df1)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "\n",
    "        plt.scatter(df1['latitude'], df1['longitude'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n",
    "        plt.show()\n",
    "        \n",
    "us_clusters = 30\n",
    "run_k_means(df_no_us, us_clusters, True)\n",
    "\n",
    "world_clusters = 30\n",
    "run_k_means(df_no_world, world_clusters, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the number of data points for the top ten clusters for the US dataframe.\")\n",
    "print(df_no_us['cluster_label'].value_counts().head(10))\n",
    "print()\n",
    "print(\"This is the number of data points for the top ten clusters for the world dataframe.\")\n",
    "print(df_no_world['cluster_label'].value_counts().head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
